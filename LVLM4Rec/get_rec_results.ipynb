{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LVLMRecommender Usage Guide\n",
    "This notebook demonstrates how to use the `LVLMRecommender` class to process samples from a dataset using either OpenAI or Claude APIs. The following steps outline the process of setting up and executing the tasks.\n",
    "\n",
    "## Step 1: Import Necessary Libraries\n",
    "\n",
    "First, import all necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "import base64\n",
    "import httpx\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Define the LVLMRecommender Class\n",
    "\n",
    "The `LVLMRecommender` class encapsulates all the functionality required to interact with the APIs and process the samples. Ensure the class definition is included in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LVLMRecommender:\n",
    "    def __init__(self, api_type, api_key, base_url, dataset_name, max_seq_len, template_id, model_name, incremental_mode=False):\n",
    "        \"\"\"\n",
    "        Initialize the LVLMRecommender with necessary parameters.\n",
    "        \"\"\"\n",
    "        self.api_type = api_type\n",
    "        self.model_name = model_name\n",
    "        self.template_id = template_id\n",
    "        self.client = self.initialize_client(api_key, base_url)\n",
    "        self.input_file = f'./prompts/sampled_prompts/{dataset_name}_{max_seq_len}/prompts_{template_id}.json'\n",
    "        self.output_path = f'./results/{dataset_name}_{max_seq_len}/prompts_{template_id}_{model_name}/'\n",
    "\n",
    "    def initialize_client(self, api_key, base_url):\n",
    "        \"\"\"\n",
    "        Initialize the API client.\n",
    "        \"\"\"\n",
    "        if self.api_type == 'openai':\n",
    "            return OpenAI(api_key=api_key, base_url=base_url)\n",
    "        elif self.api_type == 'claude':\n",
    "            return Anthropic(base_url=base_url, auth_token=api_key)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid API type. Choose 'openai' or 'claude'.\")\n",
    "\n",
    "    def read_json(self, file_path):\n",
    "        \"\"\"\n",
    "        Read and return data from a JSON file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def write_json(self, data, file_path):\n",
    "        \"\"\"\n",
    "        Write data to a JSON file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "\n",
    "    def call_api(self, content, model_name):\n",
    "        \"\"\"\n",
    "        Call the appropriate API with the provided content.\n",
    "        \"\"\"\n",
    "        if self.api_type == 'openai':\n",
    "            return self.call_openai_api(content, model_name)\n",
    "        elif self.api_type == 'claude':\n",
    "            return self.call_claude_api(content, model_name)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid API type. Choose 'openai' or 'claude'.\")\n",
    "\n",
    "    def call_openai_api(self, content, model_name):\n",
    "        \"\"\"\n",
    "        Call the OpenAI API with the provided content.\n",
    "        \"\"\"\n",
    "        content_parts = content.split(\"https://\")\n",
    "        text = content_parts[0].strip()\n",
    "\n",
    "        if len(content_parts) > 1:\n",
    "            image_url = \"https://\" + content_parts[1].strip()\n",
    "            result = [\n",
    "                {\"type\": \"text\", \"text\": text},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"detail\": \"low\", \"url\": image_url}}\n",
    "            ]\n",
    "            messages = [{'role': 'user', 'content': str(result)}]\n",
    "        else:\n",
    "            messages = [{'role': 'user', 'content': text}]\n",
    "        \n",
    "        return self.client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            temperature=0.0,\n",
    "            max_tokens=4096\n",
    "        )\n",
    "\n",
    "    def call_claude_api(self, content, model_name):\n",
    "        \"\"\"\n",
    "        Call the Claude API with the provided content.\n",
    "        \"\"\"\n",
    "        content_parts = content.split(\"https://\")\n",
    "        text = content_parts[0].strip()\n",
    "        \n",
    "        if len(content_parts) > 1:\n",
    "            image_url = \"https://\" + content_parts[1].strip()\n",
    "            image1_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
    "            response = self.client.messages.create(\n",
    "                model=model_name,\n",
    "                temperature=0.0,\n",
    "                max_tokens=1024,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"image\",\n",
    "                                \"source\": {\n",
    "                                    \"type\": \"base64\",\n",
    "                                    \"media_type\": \"image/png\",\n",
    "                                    \"data\": image1_data,\n",
    "                                },\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": text + ' Just output JSON format without any description, need to generate a complete JSON format.'\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "        else:\n",
    "            response = self.client.messages.create(\n",
    "                model=model_name,\n",
    "                temperature=0.0,\n",
    "                max_tokens=1024,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": text + ' Just output completed JSON format without any description. \\nJust output the first 30 characters of each recommendation item.'\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "        return response\n",
    "\n",
    "    def save_intermediate_results(self, processed_samples, failed_samples, processing_errors):\n",
    "        \"\"\"\n",
    "        Save intermediate results to JSON files.\n",
    "        \"\"\"\n",
    "        processed_samples_path = os.path.join(self.output_path, 'processed_data.json')\n",
    "        if os.path.exists(processed_samples_path):\n",
    "            existing_processed_samples = self.read_json(processed_samples_path)\n",
    "            existing_processed_samples.update(processed_samples)\n",
    "            processed_samples = existing_processed_samples\n",
    "\n",
    "        failed_samples_path = os.path.join(self.output_path, 'failed_samples.json')\n",
    "        if os.path.exists(failed_samples_path):\n",
    "            existing_failed_samples = self.read_json(failed_samples_path)\n",
    "            existing_failed_samples.extend([sample for sample in failed_samples if sample not in existing_failed_samples])\n",
    "            failed_samples = existing_failed_samples\n",
    "\n",
    "        processing_errors_path = os.path.join(self.output_path, 'processing_errors.json')\n",
    "        if os.path.exists(processing_errors_path):\n",
    "            existing_processing_errors = self.read_json(processing_errors_path)\n",
    "            existing_processing_errors.update(processing_errors)\n",
    "            processing_errors = existing_processing_errors\n",
    "\n",
    "        self.write_json(processed_samples, processed_samples_path)\n",
    "        self.write_json(failed_samples, failed_samples_path)\n",
    "        self.write_json(processing_errors, processing_errors_path)\n",
    "        print('Sample Updated.')\n",
    "\n",
    "    def process_sample(self, sample_data, model_name, timeout_duration=120):\n",
    "        \"\"\"\n",
    "        Process a single sample by calling the API.\n",
    "        \"\"\"\n",
    "        content = sample_data['prompt']\n",
    "        if self.template_id in [\"s-1-image\", \"s-1-title-image\", \"s-2\", \"s-3\"]:\n",
    "            content += \"\\n\" + sample_data['history']['online_combined_img_path']\n",
    "\n",
    "        response = self.call_api(content, model_name)\n",
    "        message_content = response['content'][0]['text']\n",
    "        cleaned_content = message_content.replace('```json', '').replace('```', '').strip()\n",
    "        return json.loads(cleaned_content)\n",
    "\n",
    "    def process_samples(self, prompts_data, model_name, max_retries=2, timeout_duration=120):\n",
    "        \"\"\"\n",
    "        Process multiple samples by calling the API and handling retries for failed samples.\n",
    "        \"\"\"\n",
    "        processed_samples = {}\n",
    "        failed_samples = []\n",
    "        processing_errors = {}\n",
    "\n",
    "        sample_counter = 0\n",
    "        for sample_id, sample_data in tqdm(prompts_data.items(), desc=\"Processing samples\"):\n",
    "            try:\n",
    "                api_response = self.process_sample(sample_data, model_name)\n",
    "                sample_data['api_response'] = api_response\n",
    "                processed_samples[sample_id] = sample_data\n",
    "                \n",
    "                sample_counter += 1\n",
    "                if sample_counter % 1 == 0:\n",
    "                    self.save_intermediate_results(processed_samples, failed_samples, processing_errors)\n",
    "\n",
    "            except Exception as e:\n",
    "                failed_samples.append(sample_id)\n",
    "                processing_errors[sample_id] = {'error': str(e)}\n",
    "\n",
    "                for _ in range(max_retries):\n",
    "                    try:\n",
    "                        api_response = self.process_sample(sample_data, model_name)\n",
    "                        sample_data['api_response'] = api_response\n",
    "                        processed_samples[sample_id] = sample_data\n",
    "                        failed_samples.remove(sample_id)\n",
    "                        if sample_counter % 1 == 0:\n",
    "                            self.save_intermediate_results(processed_samples, failed_samples, processing_errors)\n",
    "\n",
    "                        break\n",
    "                    except Exception as retry_error:\n",
    "                        processing_errors[sample_id] = {'retry_error': str(retry_error)}\n",
    "        self.save_intermediate_results(processed_samples, failed_samples, processing_errors)\n",
    "\n",
    "        return processed_samples, failed_samples, processing_errors\n",
    "\n",
    "    def process_samples_and_save(self, resume_from_last=False, debug_mode=False):\n",
    "        \"\"\"\n",
    "        Process samples and save the results to JSON files.\n",
    "        \"\"\"\n",
    "        prompts_data = self.read_json(self.input_file)\n",
    "\n",
    "        if debug_mode:\n",
    "            print(\"Debug mode is ON: Processing only the first 10 samples.\")\n",
    "            prompts_data = dict(list(prompts_data.items())[:3])\n",
    "\n",
    "        # Load previously processed samples if resume_from_last is True\n",
    "        processed_samples = {}\n",
    "        if resume_from_last:\n",
    "            processed_samples_path = os.path.join(self.output_path, 'processed_data.json')\n",
    "            if os.path.exists(processed_samples_path):\n",
    "                processed_samples = self.read_json(processed_samples_path)\n",
    "                print(f\"Resuming from last session. {len(processed_samples)} samples already processed.\")\n",
    "\n",
    "        # Initialize the set of already processed sample IDs\n",
    "        already_processed_ids = set(processed_samples.keys())\n",
    "            # Create the output directory if it does not exist\n",
    "        if not os.path.exists(self.output_path):\n",
    "            os.makedirs(self.output_path)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Process samples\n",
    "        new_processed_samples, failed_samples, processing_errors = self.process_samples(\n",
    "            {k: v for k, v in prompts_data.items() if k not in already_processed_ids}, \n",
    "            self.model_name\n",
    "        )\n",
    "\n",
    "        # Update the processed_samples dictionary with new data\n",
    "        processed_samples.update(new_processed_samples)\n",
    "\n",
    "        # Save the results\n",
    "        self.write_json(processed_samples, os.path.join(self.output_path, 'processed_data.json'))\n",
    "        self.write_json(failed_samples, os.path.join(self.output_path, 'failed_samples.json'))\n",
    "        self.write_json(processing_errors, os.path.join(self.output_path, 'processing_errors.json'))\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        average_time_per_sample = total_time / len(prompts_data)\n",
    "\n",
    "        return self.output_path, total_time, average_time_per_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set Up Configuration Parameters\n",
    "\n",
    "Define the configuration parameters such as API keys, base URLs, dataset names, sequence length, template IDs, and model names. These parameters will be used to instantiate the `LVLMRecommender` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"your_api_key_here\"\n",
    "base_url = \"your_base_url_here\"\n",
    "dataset_name = \"your_dataset_name_here\"\n",
    "max_seq_len = 10\n",
    "template_id = \"your_template_id_here\"\n",
    "model_name = \"your_model_name_here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define the Task Execution Function\n",
    "\n",
    "Create a function to execute a single processing task using the `LVLMRecommender` class based on the given parameters. This function will handle processing and saving the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_task(api_type, dataset_name, template_id, model_name, debug_mode):\n",
    "    \"\"\"\n",
    "    Execute a single processing task using LVLMRecommender based on the given parameters.\n",
    "    \"\"\"\n",
    "    # Instantiate the LVLMRecommender class with the given configuration parameters\n",
    "    processor = LVLMRecommender(api_type, api_key, base_url, dataset_name, max_seq_len, template_id, model_name)\n",
    "    # Call the process_samples_and_save method to process data and save the results\n",
    "    result = processor.process_samples_and_save(resume_from_last=True, debug_mode=debug_mode)\n",
    "    # Print task-related information\n",
    "    print(f\"Dataset: {dataset_name}, Template ID: {template_id}, Total time taken: {result[1]:.2f} seconds\")\n",
    "    print(f\"Dataset: {dataset_name}, Template ID: {template_id}, Average time per sample: {result[2]:.2f} seconds\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define the Tasks\n",
    "\n",
    "Specify the tasks you want to execute. Each task is a tuple containing the API type, dataset name, template ID, model name, and a debug mode flag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    ('openai', 'beauty', 'r-1', 'gpt-4o-2024-05-13', False),\n",
    "    # Add more tasks as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Execute the Tasks Using ThreadPoolExecutor\n",
    "\n",
    "Use `concurrent.futures.ThreadPoolExecutor` to run the tasks in parallel. This allows for efficient execution of multiple tasks concurrently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(tasks)) as executor:\n",
    "    # Start all tasks using a list comprehension\n",
    "    futures = [executor.submit(run_task, api_type, ds_name, tpl_id, model_name, debug_mode) for api_type, ds_name, tpl_id, model_name, debug_mode in tasks]\n",
    "\n",
    "    # Print the result of each future as it completes\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        future.result()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
