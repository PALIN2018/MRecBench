{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecommendationEvaluator Execution in Jupyter Notebook\n",
    "\n",
    "This notebook demonstrates how to use the `RecommendationEvaluator` class to evaluate recommendations from processed data files. The following steps outline the process of setting up and executing the evaluation.\n",
    "\n",
    "## Step 1: Import Necessary Libraries\n",
    "\n",
    "First, import all necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import html\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from utils.metrics4rec import evaluate_all\n",
    "import openpyxl  # Ensure this is installed for Excel handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the RecommendationEvaluator Class\n",
    "\n",
    "The `RecommendationEvaluator` class encapsulates all the functionality required to evaluate recommendations. Ensure the class definition is included in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationEvaluator:\n",
    "    def __init__(self, processed_data_path, min_recommendations=1):\n",
    "        \"\"\"\n",
    "        Initialize the RecommendationEvaluator with the path to processed data and minimum recommendations.\n",
    "        \"\"\"\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.processed_data_path = processed_data_path\n",
    "        self.min_recommendations = min_recommendations\n",
    "        self.processed_data = self.load_json_data(processed_data_path)\n",
    "        self.failed_user_ids = []  # List to store user IDs with JSONDecodeError\n",
    "\n",
    "    @staticmethod\n",
    "    def load_json_data(file_path):\n",
    "        \"\"\"\n",
    "        Load and return data from a JSON file.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "        \n",
    "    def jaccard_similarity(self, s1, s2):\n",
    "        \"\"\"\n",
    "        Calculate Jaccard similarity between two strings.\n",
    "        \"\"\"\n",
    "        set1 = set(s1.split())\n",
    "        set2 = set(s2.split())\n",
    "\n",
    "        intersection = set1.intersection(set2)\n",
    "        union = set1.union(set2)\n",
    "\n",
    "        if not union:\n",
    "            return 100.0  # Avoid division by zero\n",
    "\n",
    "        similarity = len(intersection) / len(union) * 100\n",
    "        return similarity\n",
    "\n",
    "    def find_most_similar_jaccard(self, target, recommendations, strict=False):\n",
    "        \"\"\"\n",
    "        Find the most similar recommendation to the target using Jaccard similarity.\n",
    "        \"\"\"\n",
    "        max_id = 0\n",
    "        max_sim = 0\n",
    "        for id, s2 in enumerate(recommendations):\n",
    "            sim = self.jaccard_similarity(target, s2)\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                max_id = id\n",
    "\n",
    "        if strict:\n",
    "            target_first_word = target.split()[0] if target.split() else \"\"\n",
    "            recommendation_first_word = recommendations[max_id].split()[0] if recommendations[max_id].split() else \"\"\n",
    "            if target_first_word == recommendation_first_word:\n",
    "                return max_id, max_sim\n",
    "            else:\n",
    "                return None, 0\n",
    "\n",
    "        return max_id, max_sim\n",
    "    \n",
    "    def prepare_evaluation_data(self):\n",
    "        \"\"\"\n",
    "        Prepare prediction and ground truth data for evaluation.\n",
    "        \"\"\"\n",
    "        predictions, ground_truth = {}, {}\n",
    "\n",
    "        json_failed_count = 0\n",
    "        self.failed_user_ids = []\n",
    "\n",
    "        for user_id in tqdm(self.processed_data, desc=\"Evaluating\"):\n",
    "            info = self.processed_data[user_id]\n",
    "            ground_truth[user_id] = set([html.unescape(info['target']['titles'][0])])\n",
    "\n",
    "            if \"api_response\" in info and isinstance(info['api_response'], str):\n",
    "                try:\n",
    "                    info['api_response'] = json.loads(info['api_response'])\n",
    "                except json.JSONDecodeError:\n",
    "                    json_failed_count += 1\n",
    "                    self.failed_user_ids.append(user_id)\n",
    "\n",
    "            if \"api_response\" in info and isinstance(info['api_response'], dict):\n",
    "                recommendations = info['api_response'].get('recommendations', [])\n",
    "            else:\n",
    "                recommendations = []\n",
    "\n",
    "            if len(recommendations) == 0:\n",
    "                continue\n",
    "            if recommendations:\n",
    "                recommendations = [html.unescape(_) for _ in recommendations]\n",
    "                max_id, _ = self.find_most_similar_jaccard(target=info['target']['titles'][0], recommendations=recommendations, strict=True)\n",
    "                mapped_titles = [_ for _ in recommendations]\n",
    "                if max_id is not None:\n",
    "                    mapped_titles[max_id] = list(ground_truth[user_id])[0]\n",
    "\n",
    "                predictions[user_id] = {title: len(mapped_titles) - idx for idx, title in enumerate(mapped_titles)}\n",
    "            else:\n",
    "                predictions[user_id] = {f\"Item_{i}\": self.min_recommendations - i for i in range(self.min_recommendations)}\n",
    "        print(\"failed parsed json:{}_{}\".format(json_failed_count, len(self.failed_user_ids)))\n",
    "        return predictions, ground_truth\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate recommendations and return metrics for different topks.\n",
    "        \"\"\"\n",
    "        predictions, ground_truth = self.prepare_evaluation_data()\n",
    "        metrics = {f'@{k}': evaluate_all(predictions, ground_truth, topk=k) for k in [1, 3, 5, 10, 20, 30]}\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Helper Function to Find Evaluation Paths\n",
    "\n",
    "Create a function to traverse the directory and find all paths to `processed_data.json` files within directories whose names contain specified substrings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_evaluation_paths(root_path, containing):\n",
    "    \"\"\"\n",
    "    Traverse the directory to find all paths to 'processed_data.json' files\n",
    "    within directories whose names contain specified substrings.\n",
    "    \"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        for filename in filenames:\n",
    "            if filename == 'processed_data.json' or filename == 'result.json' and any(contained in dirpath for contained in containing):\n",
    "                yield os.path.join(dirpath, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Define the Main Function to Execute the Evaluation Process\n",
    "\n",
    "Create the `main` function to execute the evaluation process for multiple datasets. This function will handle loading the data, initializing the evaluator, and saving the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(root_path, containing):\n",
    "    \"\"\"\n",
    "    Main function to execute the evaluation process for multiple datasets.\n",
    "    \"\"\"\n",
    "    # Settings and paths\n",
    "    dataset_name_list = ['beauty', 'clothing', 'toys', 'sports']\n",
    "    for dataset_name in dataset_name_list:\n",
    "        max_seq_len = 10\n",
    "        formatted_root_path = root_path.format(dataset_name, max_seq_len)\n",
    "        # Initialize variables\n",
    "        results = []\n",
    "\n",
    "        # Logging to debug path discovery\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "        for file_path_data in find_evaluation_paths(formatted_root_path, containing):\n",
    "            # Log the found paths for debugging\n",
    "            logging.info(f\"Found processed_data.json at: {file_path_data}\")\n",
    "\n",
    "            # Parse template_id and model_name from the directory name\n",
    "            dir_name_components = os.path.basename(os.path.dirname(file_path_data)).split('_')\n",
    "            template_id = dir_name_components[1]\n",
    "            model_name = '_'.join(dir_name_components[2:])\n",
    "\n",
    "            # Create evaluator and evaluate\n",
    "            evaluator = RecommendationEvaluator(file_path_data)            \n",
    "            evaluation_metrics = evaluator.evaluate()\n",
    "\n",
    "            # Prepare results for all topks\n",
    "            logging.info(evaluation_metrics)  # Log the evaluation metrics for debugging\n",
    "            result = {\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"model_name\": model_name,\n",
    "                \"max_seq_len\": max_seq_len,\n",
    "                \"template_id\": template_id,\n",
    "                \"cnt\": evaluation_metrics[f'@1'][1]['cnt']\n",
    "            }\n",
    "\n",
    "            # Extract NDCG and Hits for each k and update the result dictionary\n",
    "            for k in [1, 3, 5, 10, 20, 30]:\n",
    "                result[f\"NDCG@{k}\"] = evaluation_metrics[f'@{k}'][1]['ndcg']\n",
    "            for k in [1, 3, 5, 10, 20, 30]:\n",
    "                result[f\"Hits@{k}\"] = evaluation_metrics[f'@{k}'][1]['hit']\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "        # Convert results to a DataFrame and save as Excel\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_excel(\"{}_{}_results.xlsx\".format(dataset_name, max_seq_len), index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the root path and the containing substrings\n",
    "    root_path = input(\"Enter the root path (use placeholders {} for dataset and sequence length): \")\n",
    "    containing = input(\"Enter the substrings to search for (comma-separated): \").split(',')\n",
    "\n",
    "    # Run the main function with the defined parameters\n",
    "    main(root_path, containing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
