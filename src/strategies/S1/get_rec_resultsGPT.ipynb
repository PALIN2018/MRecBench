{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class OpenAIProcessor:\n",
    "    def __init__(self, api_key, api_base, dataset_name, max_seq_len, template_id, model_name, incremental_mode=False):\n",
    "        self.model_name = model_name\n",
    "        self.template_id = template_id\n",
    "        # print(template_id)\n",
    "        self.client = self.initialize_client(api_key, api_base)\n",
    "        self.input_file = './prompts/{}_{}/prompts_{}.json'.format(dataset_name, max_seq_len, template_id)\n",
    "        self.output_path = './test/{}_{}/prompts_{}_{}/'.format(dataset_name, max_seq_len, template_id, model_name)\n",
    "        if incremental_mode:\n",
    "            self.input_file = './incremental_prompts/{}_{}/prompts_{}.json'.format(dataset_name, max_seq_len, template_id)\n",
    "            self.output_path = './incremental_output/{}_{}/prompts_{}_{}/'.format(dataset_name, max_seq_len, template_id, model_name)\n",
    "\n",
    "    def initialize_client(self, api_key, api_base):\n",
    "        return OpenAI(api_key=api_key, base_url=api_base)\n",
    "\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def write_json(self, data, file_path):\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "\n",
    "    def call_api(self, content, model_name):\n",
    "        content_parts = content.split(\"https://\")\n",
    "        # print(content_parts)\n",
    "        text = content_parts[0].strip()\n",
    "        \n",
    "        if len(content_parts) > 1:\n",
    "            image_url = content.split(\"https://\")[1].strip()\n",
    "            result = [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": text\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"detail\": \"low\",\n",
    "                        \"url\": \"https://\" + image_url\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "            messages = [{'role': 'user', 'content': str(result)}]\n",
    "            # print(messages)\n",
    "        else:\n",
    "            messages = [{'role': 'user', 'content': text}]\n",
    "            # print(messages)\n",
    "        \n",
    "        return self.client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            temperature=0.0,\n",
    "            max_tokens=4096\n",
    "        )\n",
    "\n",
    "    \n",
    "    def save_intermediate_results(self, processed_samples, failed_samples, processing_errors):\n",
    "        # 读取并更新已处理样本\n",
    "        processed_samples_path = os.path.join(self.output_path, 'processed_data.json')\n",
    "        if os.path.exists(processed_samples_path):\n",
    "            existing_processed_samples = self.read_json(processed_samples_path)\n",
    "            existing_processed_samples.update(processed_samples)\n",
    "            processed_samples = existing_processed_samples\n",
    "\n",
    "        # 读取并更新失败样本\n",
    "        failed_samples_path = os.path.join(self.output_path, 'failed_samples.json')\n",
    "        if os.path.exists(failed_samples_path):\n",
    "            existing_failed_samples = self.read_json(failed_samples_path)\n",
    "            existing_failed_samples.extend([sample for sample in failed_samples if sample not in existing_failed_samples])\n",
    "            failed_samples = existing_failed_samples\n",
    "\n",
    "        # 读取并更新处理错误\n",
    "        processing_errors_path = os.path.join(self.output_path, 'processing_errors.json')\n",
    "        if os.path.exists(processing_errors_path):\n",
    "            existing_processing_errors = self.read_json(processing_errors_path)\n",
    "            existing_processing_errors.update(processing_errors)\n",
    "            processing_errors = existing_processing_errors\n",
    "\n",
    "        # 保存更新后的结果\n",
    "        self.write_json(processed_samples, processed_samples_path)\n",
    "        self.write_json(failed_samples, failed_samples_path)\n",
    "        self.write_json(processing_errors, processing_errors_path)\n",
    "        print('Sample Updated.')\n",
    "\n",
    "\n",
    "    def process_sample(self, sample_data, model_name, timeout_duration=120):\n",
    "        content = sample_data['prompt']\n",
    "        #print(sample_data)\n",
    "        if 's' in self.template_id or 'r-2' in self.template_id or 'r-1' in self.template_id:\n",
    "            content += \"\\n\" + sample_data['history']['online_combined_img_path']\n",
    "        elif 'r-3' in self.template_id:\n",
    "            try:\n",
    "                #history_str = json.dumps(sample_data)\n",
    "                content += \"\\n\" \n",
    "            except TypeError as e:\n",
    "                print(f\"Error: Unable to convert history to string. {str(e)}\")\n",
    "                    # 在这里进行适当的错误处理\n",
    "\n",
    "        # try:\n",
    "            # print(content)\n",
    "            # print(model_name)\n",
    "        response = self.call_api(content, model_name)\n",
    "            #print(response)\n",
    "            # signal.alarm(0)  # Disable the alarm after successful completion\n",
    "            # if response.choices:\n",
    "        message_content = response.choices[0].message.content\n",
    "        cleaned_content = message_content.replace('```json', '').replace('```', '').strip()\n",
    "        print(json.loads(cleaned_content))\n",
    "        return json.loads(cleaned_content)\n",
    "\n",
    "\n",
    "    def process_samples(self, prompts_data, model_name, max_retries=2, timeout_duration=120):\n",
    "        processed_samples = {}\n",
    "        failed_samples = []\n",
    "        processing_errors = {}\n",
    "\n",
    "        sample_counter = 0\n",
    "        for sample_id, sample_data in tqdm(prompts_data.items(), desc=\"Processing samples\"):\n",
    "            try:\n",
    "                api_response = self.process_sample(sample_data, model_name)\n",
    "                sample_data['api_response'] = api_response\n",
    "                processed_samples[sample_id] = sample_data\n",
    "                \n",
    "                sample_counter += 1\n",
    "                if sample_counter % 1 == 0:\n",
    "                    self.save_intermediate_results(processed_samples, failed_samples, processing_errors)\n",
    "\n",
    "            except Exception as e:\n",
    "                failed_samples.append(sample_id)\n",
    "                processing_errors[sample_id] = {'error': str(e)}\n",
    "\n",
    "                for _ in range(max_retries):\n",
    "                    try:\n",
    "                        api_response = self.process_sample(sample_data, model_name)\n",
    "                        sample_data['api_response'] = api_response\n",
    "                        processed_samples[sample_id] = sample_data\n",
    "                        failed_samples.remove(sample_id)\n",
    "                        if sample_counter % 1 == 0:\n",
    "                            self.save_intermediate_results(processed_samples, failed_samples, processing_errors)\n",
    "\n",
    "                        break\n",
    "                    except Exception as retry_error:\n",
    "                        processing_errors[sample_id] = {'retry_error': str(retry_error)}\n",
    "\n",
    "            # time.sleep()\n",
    "        # 保存最终结果\n",
    "        self.save_intermediate_results(processed_samples, failed_samples, processing_errors)\n",
    "\n",
    "        return processed_samples, failed_samples, processing_errors\n",
    "    \n",
    "    def process_samples_and_save(self, resume_from_last=False, debug_mode=False):\n",
    "        prompts_data = self.read_json(self.input_file)\n",
    "\n",
    "        if debug_mode:\n",
    "            print(\"Debug mode is ON: Processing only the first 10 samples.\")\n",
    "            prompts_data = dict(list(prompts_data.items())[:3])\n",
    "\n",
    "        # Load previously processed samples if resume_from_last is True\n",
    "        processed_samples = {}\n",
    "        if resume_from_last:\n",
    "            processed_samples_path = os.path.join(self.output_path, 'processed_data.json')\n",
    "            if os.path.exists(processed_samples_path):\n",
    "                processed_samples = self.read_json(processed_samples_path)\n",
    "                print(f\"Resuming from last session. {len(processed_samples)} samples already processed.\")\n",
    "\n",
    "        # Initialize the set of already processed sample IDs\n",
    "        already_processed_ids = set(processed_samples.keys())\n",
    "\n",
    "        # Create the output directory if it does not exist\n",
    "        if not os.path.exists(self.output_path):\n",
    "            os.makedirs(self.output_path)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Process samples\n",
    "        new_processed_samples, failed_samples, processing_errors = self.process_samples(\n",
    "            {k: v for k, v in prompts_data.items() if k not in already_processed_ids}, \n",
    "            self.model_name\n",
    "        )\n",
    "\n",
    "        # Update the processed_samples dictionary with new data\n",
    "        processed_samples.update(new_processed_samples)\n",
    "\n",
    "        # Save the results\n",
    "        self.write_json(processed_samples, os.path.join(self.output_path, 'processed_data.json'))\n",
    "        self.write_json(failed_samples, os.path.join(self.output_path, 'failed_samples.json'))\n",
    "        self.write_json(processing_errors, os.path.join(self.output_path, 'processing_errors.json'))\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        average_time_per_sample = total_time / len(prompts_data)\n",
    "\n",
    "        return self.output_path, total_time, average_time_per_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "# from tqdm.notebook import tqdm  # Specifically for Jupyter Notebook\n",
    "\n",
    "# Usage\n",
    "api_key = \"xxxxxx\"\n",
    "api_base = \"xxxxxx\"\n",
    "max_seq_len = 10\n",
    "\n",
    "def run_task(dataset_name, template_id, model_name, incremental_mode, debug_mode):\n",
    "    \"\"\"\n",
    "    Execute a single processing task using OpenAIProcessor based on the given parameters.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): The name of the dataset to be processed. Used to differentiate between different datasets or tasks.\n",
    "        template_id (str): The template ID used for the current task. Specifies the particular template or configuration for processing the data.\n",
    "        model_name (str): The name of the OpenAI model to be used for the processing task. Determines which pre-trained model will be used in the API requests.\n",
    "        incremental_mode (bool): Indicates whether to run in incremental mode. In incremental mode, processing will resume from where it last stopped.\n",
    "        debug_mode (bool): Indicates whether to enable debug mode. In debug mode, only 10 samples are processed to quickly verify the logic.\n",
    "\n",
    "    Returns:\n",
    "        result: A data structure containing the processing results, typically including path, processing time, etc.\n",
    "    \"\"\"\n",
    "    # Instantiate the OpenAIProcessor class with the given configuration parameters\n",
    "    processor = OpenAIProcessor(api_key, api_base, dataset_name, max_seq_len, template_id, model_name, incremental_mode)\n",
    "    # Call the process_samples_and_save method to process data and save the results\n",
    "    result = processor.process_samples_and_save(resume_from_last=True, debug_mode=debug_mode)\n",
    "    # Print task-related information\n",
    "    print(f\"Dataset: {dataset_name}, Template ID: {template_id}, Total time taken: {result[1]:.2f} seconds\")\n",
    "    print(f\"Dataset: {dataset_name}, Template ID: {template_id}, Average time per sample: {result[2]:.2f} seconds\")\n",
    "    return result\n",
    "\n",
    "\n",
    "tasks = [\n",
    "    ('beauty', 'r-1', 'gpt-4o-2024-05-13', False, False),\n",
    "    ('beauty', 'r-2', 'gpt-4o-2024-05-13', False, False),\n",
    "    ('beauty', 'r-3', 'gpt-4o-2024-05-13', False, False),\n",
    "    ('beauty', 'r-3', 'gpt-4-0125-preview', False, False),\n",
    "    ('clothing', 'r-1', 'gpt-4o-2024-05-13', False, False),\n",
    "    ('clothing', 'r-2', 'gpt-4o-2024-05-13', False, False),\n",
    "    ('clothing', 'r-3', 'gpt-4o-2024-05-13', False, False),\n",
    "    ('clothing', 'r-3', 'gpt-4-0125-preview', False, False),\n",
    "    ('sports', 'r-1', 'gpt-4o-2024-05-13', False, False),\n",
    "    ('sports', 'r-2', 'gpt-4o-2024-05-13', False, False),\n",
    "    ('sports', 'r-3', 'gpt-4o-2024-05-13', False, False),\n",
    "    ('sports', 'r-3', 'gpt-4-0125-preview', False, False),\n",
    "    ('toys', 'r-1', 'gpt-4o-2024-05-13', False, False),\n",
    "    ('toys', 'r-2', 'gpt-4o-2024-05-13', False, False),\n",
    "    ('toys', 'r-3', 'gpt-4o-2024-05-13', False, False),\n",
    "    ('toys', 'r-3', 'gpt-4-0125-preview', False, False),\n",
    "    ('sports', 'r-1', 'gpt-4o-2024-05-13', False, False),\n",
    "    ('beauty', 'r-1', 'gpt-4-vision-preview', False, False),\n",
    "    ('beauty', 'r-2', 'gpt-4-vision-preview', False, False),\n",
    "    ('beauty', 'r-3', 'gpt-4-vision-preview', False, False),\n",
    "    ('clothing', 'r-1', 'gpt-4-vision-preview', False, False),\n",
    "    ('clothing', 'r-2', 'gpt-4-vision-preview', False, False),\n",
    "    ('clothing', 'r-3', 'gpt-4-vision-preview', False, False),\n",
    "    ('sports', 'r-1', 'gpt-4-vision-preview', False, False),\n",
    "    ('sports', 'r-2', 'gpt-4-vision-preview', False, False),\n",
    "    ('sports', 'r-3', 'gpt-4-vision-preview', False, False),\n",
    "    ('toys', 'r-1', 'gpt-4-vision-preview', False, False),\n",
    "    ('toys', 'r-2', 'gpt-4-vision-preview', False, False),\n",
    "    ('toys', 'r-3', 'gpt-4-vision-preview', False, False),\n",
    "]\n",
    "\n",
    "\n",
    "# Use ThreadPoolExecutor to run tasks in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(tasks)) as executor:\n",
    "    # Start all tasks using a list comprehension\n",
    "    futures = [executor.submit(run_task, ds_name, tpl_id, model_name, incremental_mode, debug_mode) for ds_name, tpl_id, model_name, incremental_mode, debug_mode in tasks]\n",
    "\n",
    "    # Print the result of each future as it completes\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        future.result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
