{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "# from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from anthropic import Anthropic\n",
    "import base64\n",
    "import httpx\n",
    "\n",
    "class OpenAIProcessor:\n",
    "    def __init__(self, base_url, auth_token, dataset_name, max_seq_len, template_id, model_name, incremental_mode=False):\n",
    "        self.model_name = model_name\n",
    "        self.template_id = template_id\n",
    "        # print(template_id)\n",
    "        self.client = self.initialize_client(base_url, auth_token)\n",
    "        self.input_file = './prompts/{}_{}/prompts_{}.json'.format(dataset_name, max_seq_len, template_id)\n",
    "        self.output_path = './{}_{}/prompts_{}_{}/'.format(dataset_name, max_seq_len, template_id, model_name)\n",
    "        if incremental_mode:\n",
    "            self.input_file = './incremental_prompts/{}_{}/prompts_{}.json'.format(dataset_name, max_seq_len, template_id)\n",
    "            self.output_path = './incremental_output/{}_{}/prompts_{}_{}/'.format(dataset_name, max_seq_len, template_id, model_name)\n",
    "\n",
    "    def initialize_client(self, base_url, auth_token):\n",
    "        return Anthropic(base_url=base_url, auth_token=auth_token)\n",
    "\n",
    "    def read_json(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "\n",
    "    def write_json(self, data, file_path):\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(data, file, indent=4)\n",
    "\n",
    "    def call_api(self, content, model_name):\n",
    "        content_parts = content.split(\"https://\")\n",
    "        # print(content_parts)\n",
    "        text = content_parts[0].strip()\n",
    "        \n",
    "        if len(content_parts) > 1:\n",
    "            image_url = content.split(\"https://\")[1].strip()\n",
    "            image_url = \"https://\" + image_url\n",
    "            image1_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
    "            return self.client.messages.create(\n",
    "                model= model_name,\n",
    "                temperature=0.0,\n",
    "                max_tokens=1024,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"image\",\n",
    "                                \"source\": {\n",
    "                                    \"type\": \"base64\",\n",
    "                                    \"media_type\": \"image/png\",\n",
    "                                    \"data\": image1_data,\n",
    "                                },\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": text + ' Just output JSON format without any description, need to generate a complete JSON format.'\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "        else:\n",
    "            return self.client.messages.create(\n",
    "                model= model_name,\n",
    "                temperature=0.0,\n",
    "                max_tokens=1024,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": text + ' Just output completed JSON format without any description. \\nJust output the first 30 characters of each recommendation item.'\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "\n",
    "\n",
    "    \n",
    "    def save_intermediate_results(self, processed_samples, failed_samples, processing_errors):\n",
    "        # Read and update processed samples\n",
    "        processed_samples_path = os.path.join(self.output_path, 'processed_data.json')\n",
    "        if os.path.exists(processed_samples_path):\n",
    "            existing_processed_samples = self.read_json(processed_samples_path)\n",
    "            existing_processed_samples.update(processed_samples)\n",
    "            processed_samples = existing_processed_samples\n",
    "\n",
    "        # Read and update failed samples\n",
    "        failed_samples_path = os.path.join(self.output_path, 'failed_samples.json')\n",
    "        if os.path.exists(failed_samples_path):\n",
    "            existing_failed_samples = self.read_json(failed_samples_path)\n",
    "            existing_failed_samples.extend([sample for sample in failed_samples if sample not in existing_failed_samples])\n",
    "            failed_samples = existing_failed_samples\n",
    "\n",
    "        # Read and update processing errors\n",
    "        processing_errors_path = os.path.join(self.output_path, 'processing_errors.json')\n",
    "        if os.path.exists(processing_errors_path):\n",
    "            existing_processing_errors = self.read_json(processing_errors_path)\n",
    "            existing_processing_errors.update(processing_errors)\n",
    "            processing_errors = existing_processing_errors\n",
    "\n",
    "        # Save updated results\n",
    "        self.write_json(processed_samples, processed_samples_path)\n",
    "        self.write_json(failed_samples, failed_samples_path)\n",
    "        self.write_json(processing_errors, processing_errors_path)\n",
    "        print('Sample Updated.')\n",
    "        \n",
    "        \n",
    "\n",
    "    def process_sample(self, sample_data, model_name):\n",
    "        content = sample_data['prompt']\n",
    "        if 's' in self.template_id or 'r-2' in self.template_id or 'r-1' in self.template_id:\n",
    "            content += \"\\n\" + sample_data['history']['online_combined_img_path']\n",
    "        elif 'r-3' in self.template_id:\n",
    "            try:\n",
    "                content += \"\\n\"\n",
    "            except TypeError as e:\n",
    "                print(f\"Error: Unable to convert history to string. {str(e)}\")\n",
    "                # Handle the error appropriately here\n",
    "\n",
    "        try:\n",
    "            # Simulate API call delay\n",
    "            # time.sleep(5)\n",
    "            response = self.call_api(content, model_name)\n",
    "            \n",
    "            # Check the API response content\n",
    "            if response.content:\n",
    "                message_content = response.content[0].text\n",
    "                cleaned_content = message_content.replace('```json', '').replace('```', '').strip()\n",
    "                \n",
    "                return json.loads(cleaned_content)\n",
    "            else:\n",
    "                raise Exception(\"No content in response\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSONDecodeError: {str(e)}\")\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(\"Response error:\", e)\n",
    "            raise e\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def process_samples(self, prompts_data, model_name, max_retries=2):\n",
    "        processed_samples = {}\n",
    "        failed_samples = []\n",
    "        processing_errors = {}\n",
    "\n",
    "        sample_counter = 0\n",
    "        for sample_id, sample_data in tqdm(prompts_data.items(), desc=\"Processing samples\"):\n",
    "            try:\n",
    "                api_response = self.process_sample(sample_data, model_name)\n",
    "                sample_data['api_response'] = api_response\n",
    "                processed_samples[sample_id] = sample_data\n",
    "                \n",
    "                sample_counter += 1\n",
    "                if sample_counter % 1 == 0:\n",
    "                    self.save_intermediate_results(processed_samples, failed_samples, processing_errors)\n",
    "\n",
    "            except Exception as e:\n",
    "                failed_samples.append(sample_id)\n",
    "                processing_errors[sample_id] = {'error': str(e)}\n",
    "\n",
    "                for _ in range(max_retries):\n",
    "                    try:\n",
    "                        api_response = self.process_sample(sample_data, model_name)\n",
    "                        sample_data['api_response'] = api_response\n",
    "                        processed_samples[sample_id] = sample_data\n",
    "                        failed_samples.remove(sample_id)\n",
    "                        if sample_counter % 1 == 0:\n",
    "                            self.save_intermediate_results(processed_samples, failed_samples, processing_errors)\n",
    "\n",
    "                        break\n",
    "                    except Exception as retry_error:\n",
    "                        processing_errors[sample_id] = {'retry_error': str(retry_error)}\n",
    "\n",
    "            # time.sleep(10)\n",
    "        # Save final results\n",
    "        self.save_intermediate_results(processed_samples, failed_samples, processing_errors)\n",
    "\n",
    "        return processed_samples, failed_samples, processing_errors\n",
    "    \n",
    "    def process_samples_and_save(self, resume_from_last=False, debug_mode=False):\n",
    "        prompts_data = self.read_json(self.input_file)\n",
    "\n",
    "        if debug_mode:\n",
    "            print(\"Debug mode is ON: Processing only the first 10 samples.\")\n",
    "            prompts_data = dict(list(prompts_data.items())[:3])\n",
    "\n",
    "        # Load previously processed samples if resume_from_last is True\n",
    "        processed_samples = {}\n",
    "        if resume_from_last:\n",
    "            processed_samples_path = os.path.join(self.output_path, 'processed_data.json')\n",
    "            if os.path.exists(processed_samples_path):\n",
    "                processed_samples = self.read_json(processed_samples_path)\n",
    "                print(f\"Resuming from last session. {len(processed_samples)} samples already processed.\")\n",
    "\n",
    "        # Initialize the set of already processed sample IDs\n",
    "        already_processed_ids = set(processed_samples.keys())\n",
    "\n",
    "        # Create the output directory if it does not exist\n",
    "        if not os.path.exists(self.output_path):\n",
    "            os.makedirs(self.output_path)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Process samples\n",
    "        new_processed_samples, failed_samples, processing_errors = self.process_samples(\n",
    "            {k: v for k, v in prompts_data.items() if k not in already_processed_ids}, \n",
    "            self.model_name\n",
    "        )\n",
    "\n",
    "        # Update the processed_samples dictionary with new data\n",
    "        processed_samples.update(new_processed_samples)\n",
    "\n",
    "        # Save the results\n",
    "        self.write_json(processed_samples, os.path.join(self.output_path, 'processed_data.json'))\n",
    "        self.write_json(failed_samples, os.path.join(self.output_path, 'failed_samples.json'))\n",
    "        self.write_json(processing_errors, os.path.join(self.output_path, 'processing_errors.json'))\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        average_time_per_sample = total_time / len(prompts_data)\n",
    "\n",
    "        return self.output_path, total_time, average_time_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "# from tqdm.notebook import tqdm  # Specifically for Jupyter Notebook\n",
    "\n",
    "# Usage\n",
    "auth_token = \"xxxxxx\"\n",
    "base_url = \"xxxxxxx\"\n",
    "max_seq_len = 10\n",
    "\n",
    "def run_task(dataset_name, template_id, model_name, incremental_mode, debug_mode):\n",
    "    \"\"\"\n",
    "    Execute a single processing task using the OpenAIProcessor based on the given parameters.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to be processed. Used to differentiate between different datasets or tasks.\n",
    "        template_id (str): Template ID used for the current task. Specifies the particular template or configuration for processing the data.\n",
    "        model_name (str): Name of the OpenAI model to be used for the processing task. Determines which pre-trained model will be used in the API requests.\n",
    "        incremental_mode (bool): Indicates whether to run in incremental mode. In incremental mode, processing will resume from where it last stopped.\n",
    "        debug_mode (bool): Indicates whether to enable debug mode. In debug mode, only 10 samples are processed to quickly verify the logic.\n",
    "\n",
    "    Returns:\n",
    "        result: Data structure containing the processing results, typically including path, processing time, etc.\n",
    "    \"\"\"\n",
    "    # Instantiate the OpenAIProcessor class with the given configuration parameters\n",
    "    processor = OpenAIProcessor(base_url, auth_token, dataset_name, max_seq_len, template_id, model_name, incremental_mode)\n",
    "    # Call the process_samples_and_save method to process data and save the results\n",
    "    result = processor.process_samples_and_save(resume_from_last=True, debug_mode=debug_mode)\n",
    "    # Print task-related information\n",
    "    print(f\"Dataset: {dataset_name}, Template ID: {template_id}, Total time taken: {result[1]:.2f} seconds\")\n",
    "    print(f\"Dataset: {dataset_name}, Template ID: {template_id}, Average time per sample: {result[2]:.2f} seconds\")\n",
    "    return result\n",
    "\n",
    "\n",
    "tasks = [\n",
    "    ('beauty', 'r-1', 'anthropic.claude-3-opus', False, False),\n",
    "    ('beauty', 'r-2', 'anthropic.claude-3-opus', False, False),\n",
    "    ('beauty', 'r-3', 'anthropic.claude-3-opus', False, False),\n",
    "    # ('beauty', 'r-3', 'gpt-4-0125-preview', False, False),\n",
    "    ('clothing', 'r-1', 'anthropic.claude-3-opus', False, False),\n",
    "    ('clothing', 'r-2', 'anthropic.claude-3-opus', False, False),\n",
    "    ('clothing', 'r-3', 'anthropic.claude-3-opus', False, False),\n",
    "    # ('clothing', 'r-3', 'gpt-4-0125-preview', False, False),\n",
    "    ('sports', 'r-1', 'anthropic.claude-3-opus', False, False),\n",
    "    ('sports', 'r-2', 'anthropic.claude-3-opus', False, False),\n",
    "    ('sports', 'r-3', 'anthropic.claude-3-opus', False, False),\n",
    "    # ('sports', 'r-3', 'gpt-4-0125-preview', False, False),\n",
    "    ('toys', 'r-1', 'anthropic.claude-3-opus', False, False),\n",
    "    ('toys', 'r-2', 'anthropic.claude-3-opus', False, False),\n",
    "    ('toys', 'r-3', 'anthropic.claude-3-opus', False, False),\n",
    "    ('toys', 'r-3', 'gpt-4-0125-preview', False, False),\n",
    "]\n",
    "\n",
    "\n",
    "# Use ThreadPoolExecutor to run tasks in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(tasks)) as executor:\n",
    "    # Start all tasks using a list comprehension\n",
    "    futures = [executor.submit(run_task, ds_name, tpl_id, model_name, incremental_mode, debug_mode) for ds_name, tpl_id, model_name, incremental_mode, debug_mode in tasks]\n",
    "\n",
    "    # Print the result of each future as it completes\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        future.result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
